{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![PNS](http://caillau.perso.math.cnrs.fr/logo-pns.png)\n",
    "## MAM5-INUM - Commande\n",
    "# TP 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning: Taxi\n",
    "### Credits: [@felipelodur](https://github.com/felipelodur/Q-Learning-Taxi-v2)\n",
    "\n",
    "This notebook presents an implementation of the Q-learning algorithm and applies it to the Taxi problem in order to obtain the optimal Q-table values for the problem. \n",
    "\n",
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T16:15:46.693564Z",
     "start_time": "2021-01-18T16:15:46.112568Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Envinroment: [Taxi-v3](https://www.gymlibrary.dev/environments/toy_text/taxi)\n",
    "\n",
    "This task was introduced in [Dietterich2000](https://dl.acm.org/citation.cfm?id=1622268) to illustrate some issues in hierarchical reinforcement learning. There are 4 locations (labeled by different letters) and your job is to pick up the passenger at one location and drop him off in another. You receive +20 points for a successful dropoff, and lose 1 point for every timestep it takes. There is also a 10 point penalty for illegal pick-up and drop-off actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T16:15:47.033563Z",
     "start_time": "2021-01-18T16:15:46.695566Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Taxi-v3\") # Create environment\n",
    "env.render() # Show it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-table\n",
    "\n",
    "In [*Q-learning*](https://en.wikipedia.org/wiki/Q-learning) the goal is to learn a policy that will tell an agent which action to take under each possible state. The *Q-table* is responsible to store **score** values for each *(state, action)* pair. These values can be initialized with zeros or randomly, and them they are updated as you perform *exploration* in your problem domain (which helps to discover which actions leads to a better stream of rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T16:15:48.079566Z",
     "start_time": "2021-01-18T16:15:48.076563Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action size  6\n",
      "State size  500\n"
     ]
    }
   ],
   "source": [
    "# Number of possible actions\n",
    "action_size = env.action_space.n \n",
    "print(\"Action size \", action_size) \n",
    "\n",
    "# Number of possible states\n",
    "state_size = env.observation_space.n \n",
    "print(\"State size \", state_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T16:15:48.669041Z",
     "start_time": "2021-01-18T16:15:48.666037Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "qtable = np.zeros((state_size, action_size))\n",
    "print(qtable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T16:15:49.616040Z",
     "start_time": "2021-01-18T16:15:49.613038Z"
    }
   },
   "outputs": [],
   "source": [
    "episodes = 30000            # Total episodes\n",
    "max_steps = 1000            # Max steps per episode\n",
    "lr = 0.3                    # Learning rate\n",
    "decay_fac = 0.00001         # Decay learning rate each iteration\n",
    "gamma = 0.90                # Discounting rate - later rewards impact less"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration (Q-learning)\n",
    "\n",
    "Since **the goal here is to learn the optimal q-table**, that is, best policy for each (state,action) pair: we need to perform exploration only. If the goal was to learn and solve the environment as quickly as possible, it would be necessary to implement an [exploration-exploitation](http://home.deib.polimi.it/restelli/MyWebSite/pdf/rl5.pdf) strategy. This kind of strategy would not work for this goal since we want to discover the optimal values for states that would not often/never be visited via the exploration-exploitation strategy.\n",
    "\n",
    "The exploration strategy here implemented is straightforward: for each state we get to, *take an action randomly*. The randomness \"guarantees\" that we are going to visit each state action pair eventually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T16:18:08.337846Z",
     "start_time": "2021-01-18T16:15:50.267037Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode =  3000\n",
      "learning rate =  0.26999999999997\n",
      "-----------\n",
      "episode =  6000\n",
      "learning rate =  0.23999999999993998\n",
      "-----------\n",
      "episode =  9000\n",
      "learning rate =  0.20999999999990998\n",
      "-----------\n",
      "episode =  12000\n",
      "learning rate =  0.17999999999987998\n",
      "-----------\n",
      "episode =  15000\n",
      "learning rate =  0.14999999999984998\n",
      "-----------\n",
      "episode =  18000\n",
      "learning rate =  0.11999999999982693\n",
      "-----------\n",
      "episode =  21000\n",
      "learning rate =  0.08999999999983856\n",
      "-----------\n",
      "episode =  24000\n",
      "learning rate =  0.059999999999848445\n",
      "-----------\n",
      "episode =  27000\n",
      "learning rate =  0.029999999999839697\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "for episode in range(episodes):\n",
    "    \n",
    "    state = env.reset() # Reset the environment\n",
    "    done = False        # Are we done with the environment\n",
    "    lr -= decay_fac     # Decaying learning rate\n",
    "    step = 0\n",
    "    \n",
    "    if lr <= 0: # Nothing more to learn?\n",
    "        break\n",
    "        \n",
    "    for step in range(max_steps):\n",
    "        \n",
    "        # Randomly Choose an Action\n",
    "        action = env.action_space.sample()\n",
    "        \n",
    "        # Take the action -> observe new state and reward\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        # Update qtable values\n",
    "        if done == True: # If last, do not count future accumulated reward\n",
    "            qtable[state, action] = qtable[state, action]+lr*(reward+gamma*0-qtable[state,action])\n",
    "            break\n",
    "        else: # Consider accumulated reward of best decision stream\n",
    "            qtable[state, action] = qtable[state,action] # UPDATE\n",
    "        # moving states\n",
    "        state = new_state\n",
    "        \n",
    "    episode += 1\n",
    "    \n",
    "    if (episode % 3000 == 0):\n",
    "        print('episode = ', episode)\n",
    "        print('learning rate = ', lr)\n",
    "        print('-----------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Q-table\n",
    "\n",
    "Now that we have the optimal values in the Q-table, we can use it to see our agent taking the best actions in this setting.\n",
    "You can re-run the code below to see it solving different environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T16:18:15.856847Z",
     "start_time": "2021-01-18T16:18:08.338847Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "Episode Reward =  -200\n"
     ]
    }
   ],
   "source": [
    "# New environment\n",
    "state = env.reset()\n",
    "env.render()\n",
    "done = False\n",
    "total_reward = 0\n",
    "\n",
    "while(done == False):\n",
    "    action = 0 # UPDATE - Choose best action (Q-table)\n",
    "    state, reward, done, info = env.step(action) # Take action\n",
    "    total_reward += reward  # Sum of rewards\n",
    "    \n",
    "    # Display it\n",
    "    time.sleep(0.5)\n",
    "    clear_output(wait=True)\n",
    "    env.render()\n",
    "    print('Episode Reward = ', total_reward)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python ML",
   "language": "python",
   "name": "python_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
